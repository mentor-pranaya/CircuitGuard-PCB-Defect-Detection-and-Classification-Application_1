# -*- coding: utf-8 -*-
"""Training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WnakVwOwwhr_z-F3rxxMk2w0hE5if2kb
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import timm
import numpy as np
import matplotlib.pyplot as plt
import os
import glob
import seaborn as sns
from PIL import Image
import platform # Used for better compatibility on Windows/Colab

# --- 1. CONFIGURATION AND HYPERPARAMETERS ---
# Check device for training
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


ROOT_DATA_DIR = os.path.join('..', 'Data')

ROIS_DIR = os.path.join(ROOT_DATA_DIR, 'CLASSIFICATION_ROIS_128x128')

# Model Parameters
MODEL_NAME = 'efficientnet_b4'
MODEL_SAVE_PATH = 'best_efficientnet_b4.pth'
TARGET_SIZE = (128, 128)

# Training Parameters
BATCH_SIZE = 32
NUM_EPOCHS = 10
LEARNING_RATE = 1e-4


torch.manual_seed(42)
np.random.seed(42)

NUM_WORKERS = 4 if platform.system() != 'Windows' else 0


# --- 2. DATASET AND UTILITY FUNCTIONS ---

class PCBDefectDataset(Dataset):
    """
    Custom Dataset to load cropped defect images (ROIs) and map the folder name to a class label.
    """
    def __init__(self, file_list, labels, transform=None):
        self.file_list = file_list
        self.labels = labels
        self.transform = transform


        self.class_names_set = sorted(list(set(labels)))
        self.label_map = {label: i for i, label in enumerate(self.class_names_set)}
        self.num_classes = len(self.class_names_set)

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        img_path = self.file_list[idx]


        try:
            image = Image.open(img_path).convert('RGB')
        except Exception as e:

            print(f"Error loading image {img_path}: {e}")
            return torch.zeros((3, *TARGET_SIZE)), -1

        label_str = self.labels[idx]
        label = self.label_map[label_str]

        if self.transform:
            image = self.transform(image)

        return image, label

    @property
    def classes(self):
        return self.class_names_set


def load_data_paths(rois_dir):
    """Loads all image file paths and their corresponding defect class labels."""

    all_files = glob.glob(os.path.join(rois_dir, '*', '*.png'))

    if not all_files:
        raise ValueError(f"No image files found in {rois_dir}. Check the path and folder structure.")


    all_labels = [os.path.basename(os.path.dirname(f)) for f in all_files]
    return all_files, all_labels


def get_transforms(is_train):
    """Defines the preprocessing and data augmentation pipelines."""

    NORM_MEAN = [0.485, 0.456, 0.406]
    NORM_STD = [0.229, 0.224, 0.225]

    if is_train:
        # Training transforms include augmentation
        return transforms.Compose([
            transforms.Resize(TARGET_SIZE),
            # Key Augmentations for robustness:
            transforms.RandomRotation(15),
            transforms.RandomHorizontalFlip(),
            transforms.RandomVerticalFlip(),
            transforms.ColorJitter(brightness=0.1, contrast=0.1),
            transforms.ToTensor(),
            transforms.Normalize(NORM_MEAN, NORM_STD)
        ])
    else:

        return transforms.Compose([
            transforms.Resize(TARGET_SIZE),
            transforms.ToTensor(),
            transforms.Normalize(NORM_MEAN, NORM_STD)
        ])

# --- 3. MODEL AND TRAINING SETUP ---

def setup_model(num_classes, model_name):
    """
    Implements EfficientNet-B4 using PyTorch and prepares the loss function and optimizer.
    """

    model = timm.create_model(model_name, pretrained=True)


    num_ftrs = model.classifier.in_features

    model.classifier = nn.Linear(num_ftrs, num_classes)
    model.to(DEVICE)


    criterion = nn.CrossEntropyLoss()

    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    return model, criterion, optimizer

# --- 4. TRAINING AND EVALUATION FUNCTIONS ---

def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):
    """Core training loop (Module 3, Task 3)"""
    best_acc = 0.0
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

    print(f"Starting training on device: {DEVICE}...")

    for epoch in range(num_epochs):
        # --- TRAINING PHASE ---
        model.train()
        running_loss = 0.0
        running_corrects = 0
        total_train = 0

        for inputs, labels in train_loader:
            inputs = inputs.to(DEVICE)
            labels = labels.to(DEVICE)

            optimizer.zero_grad() # Clear previous gradients
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward() # Backpropagation
            optimizer.step() # Adam optimization

            running_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            running_corrects += torch.sum(preds == labels.data)
            total_train += labels.size(0)

        epoch_loss = running_loss / total_train
        epoch_acc = running_corrects.double() / total_train

        history['train_loss'].append(epoch_loss)
        history['train_acc'].append(epoch_acc.item())

        # --- VALIDATION PHASE ---
        model.eval() # Set model to evaluation mode
        val_loss = 0.0
        val_corrects = 0
        total_val = 0

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs = inputs.to(DEVICE)
                labels = labels.to(DEVICE)
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                val_corrects += torch.sum(preds == labels.data)
                total_val += labels.size(0)

        val_epoch_loss = val_loss / total_val
        val_epoch_acc = val_corrects.double() / total_val
        history['val_loss'].append(val_epoch_loss)
        history['val_acc'].append(val_epoch_acc.item())

        print(f"Epoch {epoch+1}/{num_epochs} - Train Acc: {epoch_acc:.4f}, Val Acc: {val_epoch_acc:.4f} (Loss: {val_epoch_loss:.4f})")

        #Saving the best model (Deliverable: Trained EfficientNet model)
        if val_epoch_acc > best_acc:
            best_acc = val_epoch_acc
            torch.save(model.state_dict(), MODEL_SAVE_PATH)
            print(f"*** Checkpoint saved! New best accuracy: {best_acc.item():.4f} ***")

    return history, best_acc.item()


def evaluate_model(model, data_loader):
    """Generates predictions and true labels for confusion matrix (Module 4, Deliverable)."""
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs = inputs.to(DEVICE)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return all_labels, all_preds


def load_model_for_evaluation(num_classes, model_path, model_name):
    """Loads the best saved model weights for final evaluation."""

    model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)


    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=DEVICE))
        model.to(DEVICE)
        model.eval() # Set to evaluation mode
        print(f"\nSuccessfully loaded best model from {model_path}.")
        return model
    else:
        raise FileNotFoundError(f"Model checkpoint not found at {model_path}. Run training first.")


def plot_history(history):
    """Generates Loss and Accuracy plots (Deliverable: Evaluation plots)"""
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title('Loss Over Epochs')
    plt.xlabel('Epoch')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Train Accuracy')
    plt.plot(history['val_acc'], label='Validation Accuracy')
    plt.title('Accuracy Over Epochs')
    plt.xlabel('Epoch')
    plt.legend()
    plt.show()


def plot_confusion_matrix(true_labels, predicted_labels, class_names):
    """Generates the Confusion Matrix plot (Deliverable: Confusion matrix)"""
    cm = confusion_matrix(true_labels, predicted_labels)
    plt.figure(figsize=(8, 6))

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()


# --- 5. MAIN EXECUTION BLOCK ---

if __name__ == "__main__":

    # --- Data Loading and Splitting ---
    try:
        all_files, all_labels = load_data_paths(ROIS_DIR)


        train_files, val_files, train_labels, val_labels = train_test_split(
            all_files, all_labels, test_size=0.2, stratify=all_labels, random_state=42
        )

        train_dataset = PCBDefectDataset(train_files, train_labels, transform=get_transforms(is_train=True))
        val_dataset = PCBDefectDataset(val_files, val_labels, transform=get_transforms(is_train=False))

        # Create DataLoaders
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

        num_classes = train_dataset.num_classes
        class_names = train_dataset.classes

        print(f"Data loaded successfully. Total samples: {len(all_files)}")
        print(f"Defect classes found ({num_classes}): {class_names}")

    except ValueError as e:
        print(f"\n--- ERROR: Data Loading Failed ---\n{e}")
        print("Please ensure your data is organized as: /Data/CLASSIFICATION_ROIS_128x128/[class_name]/[image.png]")
        exit()

    # --- Model Setup and Training ---
    model, criterion, optimizer = setup_model(num_classes, MODEL_NAME)
    print(f"Model {MODEL_NAME} configured. Parameters sent to {DEVICE}.")

    history, best_val_acc = train_model(model, criterion, optimizer, train_loader, val_loader, NUM_EPOCHS)

    print(f"\n--- Training Results ---")
    print(f"Target Accuracy: â‰¥ 97%")
    print(f"Best Validation Accuracy Achieved: {best_val_acc:.4f}")

    # --- Final Evaluation and Plotting (Module 4) ---
    try:
        plot_history(history)


        best_model = load_model_for_evaluation(num_classes, MODEL_SAVE_PATH, MODEL_NAME)

        # Generate predictions for the confusion matrix
        true_labels, predicted_labels = evaluate_model(best_model, val_loader)

        # Get class names for plotting (index to name map)
        reverse_map = {v: k for k, v in train_dataset.label_map.items()}
        true_class_names = [reverse_map[label] for label in true_labels]
        pred_class_names = [reverse_map[label] for label in predicted_labels]

        plot_confusion_matrix(true_class_names, pred_class_names, class_names)

    except Exception as e:
        print(f"\n--- ERROR: Evaluation/Plotting Failed ---\n{e}")
        print("Ensure you have Matplotlib and Seaborn installed (`pip install matplotlib seaborn`) and that training saved the model file.")